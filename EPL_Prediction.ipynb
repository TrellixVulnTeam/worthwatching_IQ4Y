{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data and Discarding Some"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./clean_data.csv',sep='\\t',encoding='utf-8',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will drop all the unnecessary columns of the DataFrame. The ones used for prediction will be the statistics related to the current form of the team in the league. These are the ones that will be available in real-life for prediction. Of course the predictor column, 'watch' is kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(labels=['Date','HomeTeam','AwayTeam',\n",
    "                  'FTHG','FTAG','HS','AS','HST',\n",
    "                  'AST','HTHG','HTAG','HF','AF',\n",
    "                  'FTR','HTR','HY','AY','HR',\n",
    "                  'AR','Season','WHH','WHD','WHA'],\n",
    "         inplace=True,\n",
    "         axis=1)\n",
    "# Redorder the columns so we have home statistics, then away statistics\n",
    "# then the classifier.\n",
    "data = data[['HGS','HGA','HYC','HRC','HWW','AGS','AGA','AYC','ARC','AWW','watch']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HGS</th>\n",
       "      <th>HGA</th>\n",
       "      <th>HYC</th>\n",
       "      <th>HRC</th>\n",
       "      <th>HWW</th>\n",
       "      <th>AGS</th>\n",
       "      <th>AGA</th>\n",
       "      <th>AYC</th>\n",
       "      <th>ARC</th>\n",
       "      <th>AWW</th>\n",
       "      <th>watch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3489</th>\n",
       "      <td>1.176471</td>\n",
       "      <td>1.058824</td>\n",
       "      <td>1.588235</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>1.764706</td>\n",
       "      <td>1.411765</td>\n",
       "      <td>1.882353</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.153846</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3215</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.653846</td>\n",
       "      <td>1.576923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>1.160000</td>\n",
       "      <td>1.840000</td>\n",
       "      <td>2.040000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3361</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4479</th>\n",
       "      <td>1.428571</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4672</th>\n",
       "      <td>2.538462</td>\n",
       "      <td>1.230769</td>\n",
       "      <td>1.423077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>1.269231</td>\n",
       "      <td>1.384615</td>\n",
       "      <td>1.692308</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4300</th>\n",
       "      <td>1.680000</td>\n",
       "      <td>1.160000</td>\n",
       "      <td>1.320000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.320000</td>\n",
       "      <td>1.760000</td>\n",
       "      <td>1.960000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1774</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>1.812500</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           HGS       HGA       HYC       HRC       HWW       AGS       AGA  \\\n",
       "3489  1.176471  1.058824  1.588235  0.176471  0.352941  1.764706  1.411765   \n",
       "819   1.500000  1.333333  1.833333  0.083333  0.333333  1.000000  1.000000   \n",
       "43    0.800000  1.800000  2.400000  0.000000  0.200000  2.200000  2.400000   \n",
       "3215  1.000000  1.653846  1.576923  0.076923  0.346154  1.160000  1.840000   \n",
       "3361  2.000000  1.666667  2.666667  0.000000  0.666667  0.666667  1.333333   \n",
       "4479  1.428571  1.285714  1.428571  0.000000  0.571429  0.857143  1.000000   \n",
       "4672  2.538462  1.230769  1.423077  0.000000  0.653846  1.269231  1.384615   \n",
       "4300  1.680000  1.160000  1.320000  0.080000  0.400000  1.320000  1.760000   \n",
       "1774  1.000000  1.500000  1.562500  0.062500  0.312500  1.812500  0.875000   \n",
       "1684  0.714286  1.428571  1.714286  0.000000  0.285714  1.166667  0.500000   \n",
       "\n",
       "           AYC       ARC       AWW  watch  \n",
       "3489  1.882353  0.235294  0.470588      0  \n",
       "819   1.153846  0.230769  0.153846      0  \n",
       "43    2.600000  0.000000  1.000000      0  \n",
       "3215  2.040000  0.080000  0.520000      1  \n",
       "3361  0.666667  0.333333  0.000000      0  \n",
       "4479  1.428571  0.142857  0.142857      0  \n",
       "4672  1.692308  0.038462  0.230769      1  \n",
       "4300  1.960000  0.080000  0.360000      1  \n",
       "1774  1.687500  0.000000  0.375000      0  \n",
       "1684  1.500000  0.000000  0.166667      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to classify a new datapoint based on the previous game statistics in the league and betting odds for the coming game, into worth watching or not watching. The model will be trained on the data from previous seasons. This is a situation where k-Nearest Neighbors is appropriate. The model will take a new data point, and classify it based on the k-nearest datapoints. If the k-nearest have a majority of them as worth watching, them the new one will be classified as also worth watching. This model is going to treat the data as points living in a 13-dimensional space and you can imagine points being colored red if they are worth watching and blue if not.\n",
    "\n",
    "# Implementing the Predictor by Hand\n",
    "\n",
    "Rather than using an implementation of k-Nearest Neighbors from a package like scikit-learn I will implement it using just numpy and pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function normalizes all the columns of a dataframe\n",
    "# to have mean 0 and standard deviation 1. This ensures \n",
    "# that no column has a dominant impact on the model\n",
    "def normalize(df):\n",
    "    return (df - df.mean()) / df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes the dataframe and a newpoint to classify. It returns\n",
    "# a Series object with the distance from each of the rows of the dataframe\n",
    "# to the newpoint. We use Euclidean distance\n",
    "def find_distances(df, newpoint):\n",
    "    assert type(newpoint) == pd.Series, 'The new point must be a pd.Series object'\n",
    "    assert newpoint.shape[0] == (df.shape[1] - 1), f'The point is the wrong shape'\n",
    "    distances = []\n",
    "    for i, row in df.drop('watch', axis=1).iterrows():\n",
    "        distances.append(np.linalg.norm(row - newpoint))\n",
    "    return pd.Series(distances)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a dataframe, a series of distances (found above)\n",
    "# and a value of k this will return the\n",
    "# k nearest rows of the dataframe\n",
    "def nearest(df, distances, k):\n",
    "    assert type(k) == int, 'k must be an integer'\n",
    "    assert 1 <= k <= df.shape[0], 'k must be a positive integer'\n",
    "    assert type(distances) == pd.Series, 'Distances must be a pd.Series object'\n",
    "    copy = df.copy()\n",
    "    copy['distances'] = distances\n",
    "    return copy.sort_values(by='distances',axis=0)[:k].drop('distances', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a dataframe with k rows will return the majority\n",
    "# ruling on 'watch' variable, in case of a tie it will return\n",
    "# 0 = 'not worth watching'\n",
    "def worth_watching(nearest):\n",
    "    average = nearest['watch'].sum() / nearest['watch'].count()\n",
    "    return int(average > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can put these inside a general predcition funciton\n",
    "def predict(dataframe, newpoint, k):\n",
    "    df = normalize(dataframe)\n",
    "    # We must apply the same normalizing function to the new point\n",
    "    normed_point = (newpoint - dataframe.mean()) / dataframe.std()\n",
    "    # Drop the column it has picked up from the normalization\n",
    "    normed_point.drop('watch', inplace=True)\n",
    "    dists = find_distances(df, normed_point)\n",
    "    near = nearest(df, dists, k)\n",
    "    return worth_watching(near)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1, watch=1\n",
      "k=2, watch=0\n",
      "k=3, watch=1\n",
      "k=4, watch=0\n",
      "k=5, watch=0\n",
      "k=6, watch=0\n",
      "k=7, watch=0\n",
      "k=8, watch=0\n",
      "k=9, watch=0\n",
      "k=10, watch=0\n",
      "k=11, watch=0\n",
      "k=12, watch=0\n",
      "k=13, watch=0\n",
      "k=14, watch=0\n",
      "k=15, watch=0\n",
      "k=16, watch=0\n",
      "k=17, watch=0\n",
      "k=18, watch=0\n",
      "k=19, watch=0\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "newpoint = pd.Series({'HGS':3,'HGA':0,'HYC':0,\n",
    "                      'HRC':0,'HWW':1,'AGS':3.5,\n",
    "                      'AGA':2.7,'AYC':2.5,'ARC':0,\n",
    "                      'AWW':1})\n",
    "for k in range(1, 20):\n",
    "    print(f'k={k}, watch={predict(data, newpoint, k)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Testing\n",
    "\n",
    "To determine the best choice of k-value for the model we will be splitting the data into training and testing subsets. I will assign 80% to training and 20% to testing. These will be randomly assigned. I will run 10-fold cross validation which means that the training subset will be split into 10 same size non-intersecting subsets. We will fit the model for values of k on 90% of the training data then validate it on the remaining 10%. Once we have done this 10 times we will have an accuracy for that value of k. Once we have done this for all values of k the one with the highest accuracy from 10-fold cross-validation will be selected as the optimal value.\n",
    "\n",
    "Once we have tested all the values of k we will assess the final accuracy of the model by fitting on all the training data and testing on the test data. This process eliminates bias in the final accuracy value.\n",
    "\n",
    "## Subsetting\n",
    "\n",
    "You will notice below that the training fraction is only 10% of the data! This is because the prediction takes a very long time to run. This is due to the fact that when we are classifying a new data point we must calculate distance from this new point to all of the training points. In this kind of implementation there are no ways around it. I am going to use 10% for my own implementation and then we will use a package below for the full 80% training. It will be interesting to see the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2718) # Euler\n",
    "training_frac = 0.1 # Train on over 600 games\n",
    "testing_frac = 0.05 # Test on over 300 games\n",
    "indices_train = random.sample([i for i in range(len(data))], \n",
    "                              math.floor(training_frac * len(data)))\n",
    "indices_test = random.sample([i for i in range(len(data)) if i not in indices_train], \n",
    "                             math.floor(testing_frac * len(data)))\n",
    "data_train = data.iloc[indices_train].reset_index()\n",
    "data_test = data.iloc[indices_test].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the optimal value of k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fold done for k=1. 1 fold done for k=1. 2 fold done for k=1. 3 fold done for k=1. 4 fold done for k=1. 5 fold done for k=1. 6 fold done for k=1. 7 fold done for k=1. 8 fold done for k=1. 9 fold done for k=1. \n",
      "The validation accuracy for k=1 is 60.096% and took 191.13221383094788 seconds to run\n",
      "0 fold done for k=2. 1 fold done for k=2. 2 fold done for k=2. 3 fold done for k=2. 4 fold done for k=2. 5 fold done for k=2. 6 fold done for k=2. 7 fold done for k=2. 8 fold done for k=2. 9 fold done for k=2. \n",
      "The validation accuracy for k=2 is 64.263% and took 189.2847671508789 seconds to run\n",
      "0 fold done for k=3. 1 fold done for k=3. 2 fold done for k=3. 3 fold done for k=3. 4 fold done for k=3. 5 fold done for k=3. 6 fold done for k=3. 7 fold done for k=3. 8 fold done for k=3. 9 fold done for k=3. \n",
      "The validation accuracy for k=3 is 60.737% and took 185.27041006088257 seconds to run\n",
      "0 fold done for k=4. 1 fold done for k=4. 2 fold done for k=4. 3 fold done for k=4. 4 fold done for k=4. 5 fold done for k=4. 6 fold done for k=4. 7 fold done for k=4. 8 fold done for k=4. 9 fold done for k=4. \n",
      "The validation accuracy for k=4 is 63.782% and took 186.97909259796143 seconds to run\n",
      "0 fold done for k=5. 1 fold done for k=5. 2 fold done for k=5. 3 fold done for k=5. 4 fold done for k=5. 5 fold done for k=5. 6 fold done for k=5. 7 fold done for k=5. 8 fold done for k=5. 9 fold done for k=5. \n",
      "The validation accuracy for k=5 is 58.173% and took 193.09919500350952 seconds to run\n",
      "0 fold done for k=6. 1 fold done for k=6. 2 fold done for k=6. 3 fold done for k=6. 4 fold done for k=6. 5 fold done for k=6. 6 fold done for k=6. 7 fold done for k=6. 8 fold done for k=6. 9 fold done for k=6. \n",
      "The validation accuracy for k=6 is 64.103% and took 202.62025380134583 seconds to run\n",
      "0 fold done for k=7. 1 fold done for k=7. 2 fold done for k=7. 3 fold done for k=7. 4 fold done for k=7. 5 fold done for k=7. 6 fold done for k=7. 7 fold done for k=7. 8 fold done for k=7. 9 fold done for k=7. \n",
      "The validation accuracy for k=7 is 61.538% and took 194.9617097377777 seconds to run\n",
      "0 fold done for k=8. 1 fold done for k=8. 2 fold done for k=8. 3 fold done for k=8. 4 fold done for k=8. 5 fold done for k=8. 6 fold done for k=8. 7 fold done for k=8. 8 fold done for k=8. 9 fold done for k=8. \n",
      "The validation accuracy for k=8 is 66.026% and took 193.3092532157898 seconds to run\n",
      "0 fold done for k=9. 1 fold done for k=9. 2 fold done for k=9. 3 fold done for k=9. 4 fold done for k=9. 5 fold done for k=9. 6 fold done for k=9. 7 fold done for k=9. 8 fold done for k=9. 9 fold done for k=9. \n",
      "The validation accuracy for k=9 is 66.827% and took 173.1552939414978 seconds to run\n",
      "0 fold done for k=10. 1 fold done for k=10. 2 fold done for k=10. 3 fold done for k=10. 4 fold done for k=10. 5 fold done for k=10. 6 fold done for k=10. 7 fold done for k=10. 8 fold done for k=10. 9 fold done for k=10. \n",
      "The validation accuracy for k=10 is 67.788% and took 174.17282724380493 seconds to run\n",
      "0 fold done for k=11. 1 fold done for k=11. 2 fold done for k=11. 3 fold done for k=11. 4 fold done for k=11. 5 fold done for k=11. 6 fold done for k=11. 7 fold done for k=11. 8 fold done for k=11. 9 fold done for k=11. \n",
      "The validation accuracy for k=11 is 66.987% and took 194.90900659561157 seconds to run\n",
      "0 fold done for k=12. 1 fold done for k=12. 2 fold done for k=12. 3 fold done for k=12. 4 fold done for k=12. 5 fold done for k=12. 6 fold done for k=12. 7 fold done for k=12. 8 fold done for k=12. 9 fold done for k=12. \n",
      "The validation accuracy for k=12 is 66.667% and took 187.07621121406555 seconds to run\n",
      "0 fold done for k=13. 1 fold done for k=13. 2 fold done for k=13. 3 fold done for k=13. 4 fold done for k=13. 5 fold done for k=13. 6 fold done for k=13. 7 fold done for k=13. 8 fold done for k=13. 9 fold done for k=13. \n",
      "The validation accuracy for k=13 is 67.628% and took 186.72131943702698 seconds to run\n",
      "0 fold done for k=14. 1 fold done for k=14. 2 fold done for k=14. 3 fold done for k=14. 4 fold done for k=14. 5 fold done for k=14. 6 fold done for k=14. 7 fold done for k=14. 8 fold done for k=14. 9 fold done for k=14. \n",
      "The validation accuracy for k=14 is 66.186% and took 188.10967326164246 seconds to run\n",
      "0 fold done for k=15. 1 fold done for k=15. 2 fold done for k=15. 3 fold done for k=15. 4 fold done for k=15. 5 fold done for k=15. 6 fold done for k=15. 7 fold done for k=15. 8 fold done for k=15. 9 fold done for k=15. \n",
      "The validation accuracy for k=15 is 65.865% and took 183.66169118881226 seconds to run\n",
      "0 fold done for k=16. 1 fold done for k=16. 2 fold done for k=16. 3 fold done for k=16. 4 fold done for k=16. 5 fold done for k=16. 6 fold done for k=16. 7 fold done for k=16. 8 fold done for k=16. 9 fold done for k=16. \n",
      "The validation accuracy for k=16 is 66.667% and took 182.80310988426208 seconds to run\n",
      "0 fold done for k=17. 1 fold done for k=17. 2 fold done for k=17. 3 fold done for k=17. 4 fold done for k=17. 5 fold done for k=17. 6 fold done for k=17. 7 fold done for k=17. 8 fold done for k=17. 9 fold done for k=17. \n",
      "The validation accuracy for k=17 is 66.667% and took 190.8132507801056 seconds to run\n",
      "0 fold done for k=18. 1 fold done for k=18. 2 fold done for k=18. 3 fold done for k=18. 4 fold done for k=18. 5 fold done for k=18. 6 fold done for k=18. 7 fold done for k=18. 8 fold done for k=18. 9 fold done for k=18. \n",
      "The validation accuracy for k=18 is 67.147% and took 180.70159602165222 seconds to run\n",
      "0 fold done for k=19. 1 fold done for k=19. 2 fold done for k=19. 3 fold done for k=19. 4 fold done for k=19. 5 fold done for k=19. 6 fold done for k=19. 7 fold done for k=19. 8 fold done for k=19. 9 fold done for k=19. \n",
      "The validation accuracy for k=19 is 66.987% and took 174.29174876213074 seconds to run\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "for k in range(1, 20):\n",
    "    start = time.time()\n",
    "    correct = 0\n",
    "    for i in range(10):\n",
    "        validate_indices_low = math.floor(i / 10 * len(data_train))\n",
    "        validate_indices_high = math.floor((i + 1) / 10 * len(data_train))\n",
    "        data_validate = data_train.iloc[range(validate_indices_low, \n",
    "                                              validate_indices_high)]\n",
    "        data_fit = data_train.iloc[[i for i in range(len(data_train)) \n",
    "                                    if i not in range(validate_indices_low, \n",
    "                                                      validate_indices_high)]]\n",
    "        watch_pred = pd.DataFrame(data_validate['watch'])\n",
    "        watch_pred['pred'] = data_validate.apply(lambda row: predict(data_fit, row.drop('watch'), k), axis=1)\n",
    "        correct += len(watch_pred[watch_pred['watch'] == watch_pred['pred']])\n",
    "        print(f'{i} fold done for k={k}', end='. ')\n",
    "    accuracy.append(correct / len(data_train))\n",
    "    end = time.time()\n",
    "    print(f'\\nThe validation accuracy for k={k} is {round(accuracy[k - 1] * 100, 3)}% and took {end - start} seconds to run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing\n",
    "\n",
    "As can be seen from the output above, the optimal value of $k$ (having only checked 1 to 19) on the training/validation data was $k = 10$. We will finally test the model on the reserved testing data to get a true final accuracy. The values in the confusion matrix have also been presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_k = 10\n",
    "watch_pred = pd.DataFrame(data_test['watch'])\n",
    "watch_pred['pred'] = data_test.apply(lambda row: predict(data_train, row.drop('watch'), final_k), axis=1)\n",
    "correct = len(watch_pred[watch_pred['watch'] == watch_pred['pred']])\n",
    "accuracy = correct / len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy of the model on testing data is: 0.6538461538461539\n",
      "True positives: 11\n",
      "True negatives: 193\n",
      "False positives: 12\n",
      "False negatives: 96\n",
      "Total number of games: 312\n"
     ]
    }
   ],
   "source": [
    "print(f'Final accuracy of the model on testing data is: {accuracy}')\n",
    "true_positives = watch_pred.loc[(watch_pred['watch'] == 1) & (watch_pred['pred'] == 1)]\n",
    "true_negatives = watch_pred.loc[(watch_pred['watch'] == 0) & (watch_pred['pred'] == 0)]\n",
    "false_positives = watch_pred.loc[(watch_pred['watch'] == 0) & (watch_pred['pred'] == 1)]\n",
    "false_negatives = watch_pred.loc[(watch_pred['watch'] == 1) & (watch_pred['pred'] == 0)]\n",
    "print(f'True positives: {len(true_positives)}')\n",
    "print(f'True negatives: {len(true_negatives)}')\n",
    "print(f'False positives: {len(false_positives)}')\n",
    "print(f'False negatives: {len(false_negatives)}')\n",
    "print(f'Total number of games: {len(data_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we end with a final accuracy of 65.4% which is not bad at all, considering we are predicting sporting events which have a lot of randomness to their results. But remember, we only used 10% of the data for training and validation and then 5% for testing, so we can't really claim that the k-Nearest Neighbors will work this well in practice but the results are encouraging.\n",
    "\n",
    "On the other hand, looking at the values from the confusion matrix is little more concerning. There are only 11 true positives out of the 312 games tested. This means that over roughly 30 weekends of football, at most 11 will have a game that is predicted to be worth watching and actually be worth watching.\n",
    "\n",
    "Even more of a problem is the number of false positives. These would be games the model tells us to watch but turn out to be dull."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Using Scikit-Learn\n",
    "\n",
    "We wil be able to use the full 80% of the training/validation data with the functions of Scikit-Learn as they are made by people who know a lot more about code optimisation that myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2718) # Euler\n",
    "training_frac = 0.8\n",
    "testing_frac = 0.2\n",
    "indices_train = random.sample([i for i in range(len(data))], \n",
    "                              math.floor(training_frac * len(data)))\n",
    "indices_test = random.sample([i for i in range(len(data)) if i not in indices_train], \n",
    "                             math.floor(testing_frac * len(data)))\n",
    "data_train = data.iloc[indices_train].reset_index()\n",
    "data_test = data.iloc[indices_test].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 91.0 seconds to run\n",
      "The best validation accuracy was for k=97 and is 0.649119295436349\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "best_accuracy = 0\n",
    "start = time.time()\n",
    "for k in range(1, 100):\n",
    "    # By passing distance we give closer points a higher weight\n",
    "    # I have set the algorithm to brute meaning that the distance to \n",
    "    # all points will be checked. As I have above\n",
    "    model = KNeighborsClassifier(k, weights='distance',algorithm='brute')\n",
    "    correct = 0\n",
    "    for i in range(10):\n",
    "        validate_indices_low = math.floor(i / 10 * len(data_train))\n",
    "        validate_indices_high = math.floor((i + 1) / 10 * len(data_train))\n",
    "        data_validate = data_train.iloc[range(validate_indices_low, \n",
    "                                              validate_indices_high)]\n",
    "        data_fit = data_train.iloc[[i for i in range(len(data_train)) \n",
    "                                    if i not in range(validate_indices_low, \n",
    "                                                      validate_indices_high)]]\n",
    "        watch_pred = pd.DataFrame(data_validate['watch'])\n",
    "        model.fit(data_fit.drop('watch', axis=1), data_fit['watch'])\n",
    "        watch_pred['pred'] = model.predict(data_validate.drop('watch', axis=1))\n",
    "        correct += len(watch_pred[watch_pred['watch'] == watch_pred['pred']])\n",
    "    if correct / len(data_train) > best_accuracy:\n",
    "        best_accuracy = correct / len(data_train)\n",
    "        best_k = k\n",
    "end = time.time()\n",
    "print(f'This took {round(end - start, 0)} seconds to run')\n",
    "print(f'The best validation accuracy was for k={best_k} and is {best_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the testing data for k=97 is 66.133\n",
      "[[794  47]\n",
      " [376  32]]\n",
      "Total number of games 1249\n"
     ]
    }
   ],
   "source": [
    "final_k = 97\n",
    "# Same parameters as above\n",
    "model = KNeighborsClassifier(final_k, weights='distance', algorithm='brute')\n",
    "watch_pred = pd.DataFrame(data_test['watch'])\n",
    "# This time we fit on all the training data\n",
    "model.fit(data_train.drop('watch', axis=1), data_train['watch'])\n",
    "# Predict on the remaining, unseen 20% of testing data\n",
    "watch_pred['pred'] = model.predict(data_test.drop('watch', axis=1))\n",
    "correct = len(watch_pred[watch_pred['watch'] == watch_pred['pred']])\n",
    "accuracy = correct / len(data_test)\n",
    "print(f'The accuracy on the testing data for k={final_k} is {round(accuracy * 100, 3)}')\n",
    "print(confusion_matrix(y_true = watch_pred['watch'], y_pred=watch_pred['pred']))\n",
    "print(f'Total number of games {len(data_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above show that the k-Nearest Neighbors classifier is performing just as well as my implementation but has selected a much higher value of k as optimal. To run my implmentation of k-nearest neighbors for $k=97$ would take a very long time. Again, we get dissapointing results from the confusion matrix. The number of true positives is only 32 out of a total of 1249 games! There are many false negatives which means missed opportunities to watch exciting games and many false positives.\n",
    "\n",
    "**So despite a good final accuracy for the model, it may not work so well in practice. Can we do better?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
