{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.kernel_approximation import Nystroem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('clean_data.csv', sep='\\t')\n",
    "data = data[['Date','WHH','WHD','WHA','HWW','AWW','watch']]\n",
    "data = data.reindex(index=data.index[::-1])\n",
    "data.reset_index(inplace=True)\n",
    "data['Date'] =  pd.to_datetime(data['Date'])\n",
    "data[['WHH','WHD','WHA','HWW','AWW','watch']] = data[['WHH','WHD','WHA','HWW','AWW','watch']].apply(pd.to_numeric)\n",
    "data.drop('index',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent = data[data['Date'] > '2010-08-01']\n",
    "recent = data[data['Date'] < '2018-05-13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = recent[['WHH','WHD','WHA','HWW','AWW']]\n",
    "y = recent['watch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below the data is split into training, validation and testing. Sklearn's train_test_split can be used twice to partition randomly. We are using the 70-15-15% split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_vt, y_train, y_vt = train_test_split(X, y, test_size=0.3, random_state=101)\n",
    "X_validate, X_test, y_validate, y_test = train_test_split(X_vt, y_vt, test_size=0.5, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Various Support Vector Machine Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first try a support vector machine model with up to degree 5 features and a linear decision boundary. The parameter C optimized over values from 0.00001 to 10000 in powers of 10. This parameter C can be thought of as the penalization of misclassification. If C is very high then the classifier will be punished greatly for wrong classifications and will adjust to make as many as possible correct. This can of course, lead to overfitting the data. We will see whether this is the case when using the test set.\n",
    "\n",
    "The standard scaler was used which means that the features were scaled to have mean zero and standard deviation 1. This is very important for SVMs because if there is a great difference in scale then the algorithm to solve can fail.\n",
    "\n",
    "The result we are interested in is if the model predicts a game to be worth watching, the probability that it actually is interesting. This corresponds to $\\frac{TP}{FP + TP}$ from the confusion matrix and is usually referred to as **precision**. \n",
    "\n",
    "However, some models can have a good precision but hardly predict any games to be worth watching. Therefore, we require $TP > 100$.\n",
    "\n",
    "I have decided to suppress warnings about the lack of convergence.\n",
    "\n",
    "The best classifier of this type had $c=0.0001$ and had precision of 39.8%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with d=3, c=1e-05\n",
      "done with d=3, c=0.0001\n",
      "done with d=3, c=0.001\n",
      "done with d=3, c=0.01\n",
      "done with d=3, c=0.1\n",
      "done with d=3, c=1\n",
      "done with d=3, c=10\n",
      "done with d=3, c=100\n",
      "done with d=3, c=1000\n",
      "done with d=3, c=10000\n",
      "done with d=3, c=100000\n",
      "done with d=4, c=1e-05\n",
      "done with d=4, c=0.0001\n",
      "done with d=4, c=0.001\n",
      "done with d=4, c=0.01\n",
      "done with d=4, c=0.1\n",
      "done with d=4, c=1\n",
      "done with d=4, c=10\n",
      "done with d=4, c=100\n",
      "done with d=4, c=1000\n",
      "done with d=4, c=10000\n",
      "done with d=4, c=100000\n",
      "done with d=5, c=1e-05\n",
      "done with d=5, c=0.0001\n",
      "done with d=5, c=0.001\n",
      "done with d=5, c=0.01\n",
      "done with d=5, c=0.1\n",
      "done with d=5, c=1\n",
      "done with d=5, c=10\n",
      "done with d=5, c=100\n",
      "done with d=5, c=1000\n",
      "done with d=5, c=10000\n",
      "done with d=5, c=100000\n",
      "done with d=6, c=1e-05\n",
      "done with d=6, c=0.0001\n",
      "done with d=6, c=0.001\n",
      "done with d=6, c=0.01\n",
      "done with d=6, c=0.1\n",
      "done with d=6, c=1\n",
      "done with d=6, c=10\n",
      "done with d=6, c=100\n",
      "done with d=6, c=1000\n",
      "done with d=6, c=10000\n",
      "done with d=6, c=100000\n",
      "done with d=7, c=1e-05\n",
      "done with d=7, c=0.0001\n",
      "done with d=7, c=0.001\n",
      "done with d=7, c=0.01\n",
      "done with d=7, c=0.1\n",
      "done with d=7, c=1\n",
      "done with d=7, c=10\n",
      "done with d=7, c=100\n",
      "done with d=7, c=1000\n",
      "done with d=7, c=10000\n",
      "done with d=7, c=100000\n",
      "[[455 145]\n",
      " [235 101]]\n",
      "0.4105691056910569\n",
      "4\n",
      "1e-05\n"
     ]
    }
   ],
   "source": [
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def linear_svc():\n",
    "    best_precision = 0\n",
    "    for d in range(3, 8):\n",
    "        for c in [10**i for i in range(-5, 6)]:\n",
    "            polynomial_svm_clf = Pipeline([\n",
    "                ('poly_features', PolynomialFeatures(degree=d)),\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('svm_clf', LinearSVC(C=c, loss='hinge', random_state=101))\n",
    "            ])\n",
    "\n",
    "            polynomial_svm_clf.fit(X_train, y_train)\n",
    "            preds = polynomial_svm_clf.predict(X_validate)\n",
    "            cf = confusion_matrix(y_validate, preds)\n",
    "            if cf[1][1] > 100:\n",
    "                if cf[1][1] / (cf[1][1] + cf[0][1]) > best_precision:\n",
    "                    best_cf = cf\n",
    "                    best_precision = cf[1][1] / (cf[1][1] + cf[0][1])\n",
    "                    best_c = c\n",
    "                    best_d = d\n",
    "            print(f'done with d={d}, c={c}')\n",
    "    print(best_cf)\n",
    "    print(best_precision)\n",
    "    print(best_d)\n",
    "    print(best_c)\n",
    "linear_svc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I wanted to try an SVM with a polynomial kernel. If I just used an SVC with the kernel set as `poly` the model would take a very long time to fit. This is because fitting the model boils down to a quadratic optimization problem which runs in roughly $O(n_{features} \\times n^{3}_{observations})$ time.\n",
    "\n",
    "Therefore, we avoid this problem by employing what is called the \"kernel approximation\". Roughly speaking, rather than using a complex kernel, we transform the features in such a way to approximate the desired kernel, then solve using the linear kernel (which is very fast to solve). \n",
    "\n",
    "This is done for degrees 3 to 7 polynomials and for C values from 0.01 to 10,000 in powers of 10. The best result was for degree 7 and C value 100. This gave a precision percentage of 39.7%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with c=0.01, d=3\n",
      "done with c=0.01, d=4\n",
      "done with c=0.01, d=5\n",
      "done with c=0.01, d=6\n",
      "done with c=0.01, d=7\n",
      "done with c=0.1, d=3\n",
      "done with c=0.1, d=4\n",
      "done with c=0.1, d=5\n",
      "done with c=0.1, d=6\n",
      "done with c=0.1, d=7\n",
      "done with c=1, d=3\n",
      "done with c=1, d=4\n",
      "done with c=1, d=5\n",
      "done with c=1, d=6\n",
      "done with c=1, d=7\n",
      "done with c=10, d=3\n",
      "done with c=10, d=4\n",
      "done with c=10, d=5\n",
      "done with c=10, d=6\n",
      "done with c=10, d=7\n",
      "done with c=100, d=3\n",
      "done with c=100, d=4\n",
      "done with c=100, d=5\n",
      "done with c=100, d=6\n",
      "done with c=100, d=7\n",
      "done with c=1000, d=3\n",
      "done with c=1000, d=4\n",
      "done with c=1000, d=5\n",
      "done with c=1000, d=6\n",
      "done with c=1000, d=7\n",
      "done with c=10000, d=3\n",
      "done with c=10000, d=4\n",
      "done with c=10000, d=5\n",
      "done with c=10000, d=6\n",
      "done with c=10000, d=7\n",
      "[[355 245]\n",
      " [175 161]]\n",
      "0.39655172413793105\n",
      "100\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def poly_svc():\n",
    "    best_precision = 0\n",
    "    for c in [10**i for i in range(-2, 5)]:\n",
    "        for d in range(3, 8):\n",
    "            kernel_approx = Nystroem(kernel='poly',\n",
    "                                         degree=d,\n",
    "                                         random_state=101)\n",
    "            poly_svm_clf = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('svm_clf', LinearSVC(C=c, loss='hinge', random_state=101))\n",
    "                ])\n",
    "\n",
    "            X_train_transformed = kernel_approx.fit_transform(X_train)\n",
    "            X_validate_transformed = kernel_approx.fit_transform(X_validate)\n",
    "            poly_svm_clf.fit(X_train_transformed, y_train)\n",
    "            preds = poly_svm_clf.predict(X_validate_transformed)\n",
    "            cf = confusion_matrix(y_validate, preds)\n",
    "            if cf[1][1] > 100:\n",
    "                if cf[1][1] / (cf[1][1] + cf[0][1]) > best_precision:\n",
    "                    best_cf = cf\n",
    "                    best_precision = cf[1][1] / (cf[1][1] + cf[0][1])\n",
    "                    best_c = c\n",
    "                    best_d = d\n",
    "            print(f'done with c={c}, d={d}')\n",
    "    print(best_cf)\n",
    "    print(best_precision)\n",
    "    print(best_c)\n",
    "    print(best_d)\n",
    "poly_svc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the rbf kernel was used with the same kernel approximation method as above. The Gaussian RBF kernel function is given by:\n",
    "$$\\phi_\\gamma (\\textbf{x}, \\ell) = \\exp(-\\gamma \\| \\textbf{x} - \\ell \\| ^ 2)$$\n",
    "\n",
    "This kernel can handle very complicated decision boundaries.\n",
    "\n",
    "The best result was: a probability of 37.7% with a c=10 and gamma=0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with c=0.01, g=0.01\n",
      "done with c=0.01, g=0.1\n",
      "done with c=0.01, g=1\n",
      "done with c=0.01, g=10\n",
      "done with c=0.01, g=100\n",
      "done with c=0.01, g=1000\n",
      "done with c=0.01, g=10000\n",
      "done with c=0.01, g=100000\n",
      "done with c=0.1, g=0.01\n",
      "done with c=0.1, g=0.1\n",
      "done with c=0.1, g=1\n",
      "done with c=0.1, g=10\n",
      "done with c=0.1, g=100\n",
      "done with c=0.1, g=1000\n",
      "done with c=0.1, g=10000\n",
      "done with c=0.1, g=100000\n",
      "done with c=1, g=0.01\n",
      "done with c=1, g=0.1\n",
      "done with c=1, g=1\n",
      "done with c=1, g=10\n",
      "done with c=1, g=100\n",
      "done with c=1, g=1000\n",
      "done with c=1, g=10000\n",
      "done with c=1, g=100000\n",
      "done with c=10, g=0.01\n",
      "done with c=10, g=0.1\n",
      "done with c=10, g=1\n",
      "done with c=10, g=10\n",
      "done with c=10, g=100\n",
      "done with c=10, g=1000\n",
      "done with c=10, g=10000\n",
      "done with c=10, g=100000\n",
      "done with c=100, g=0.01\n",
      "done with c=100, g=0.1\n",
      "done with c=100, g=1\n",
      "done with c=100, g=10\n",
      "done with c=100, g=100\n",
      "done with c=100, g=1000\n",
      "done with c=100, g=10000\n",
      "done with c=100, g=100000\n",
      "done with c=1000, g=0.01\n",
      "done with c=1000, g=0.1\n",
      "done with c=1000, g=1\n",
      "done with c=1000, g=10\n",
      "done with c=1000, g=100\n",
      "done with c=1000, g=1000\n",
      "done with c=1000, g=10000\n",
      "done with c=1000, g=100000\n",
      "done with c=10000, g=0.01\n",
      "done with c=10000, g=0.1\n",
      "done with c=10000, g=1\n",
      "done with c=10000, g=10\n",
      "done with c=10000, g=100\n",
      "done with c=10000, g=1000\n",
      "done with c=10000, g=10000\n",
      "done with c=10000, g=100000\n",
      "[[390 210]\n",
      " [209 127]]\n",
      "0.3768545994065282\n",
      "10\n",
      "0.01\n"
     ]
    }
   ],
   "source": [
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def rbf_svc():\n",
    "    best_precision = 0\n",
    "    for c in [10**i for i in range(-2, 5)]:\n",
    "        for g in [10**i for i in range(-2, 6)]:\n",
    "            kernel_approx = Nystroem(kernel='rbf',\n",
    "                                     gamma=g,\n",
    "                                     random_state=101)\n",
    "            rbf_svm_clf = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('svm_clf', LinearSVC(C=c, loss='hinge', random_state=101))\n",
    "                ])\n",
    "            X_train_transformed = kernel_approx.fit_transform(X_train)\n",
    "            X_validate_transformed = kernel_approx.fit_transform(X_validate)\n",
    "            rbf_svm_clf.fit(X_train_transformed, y_train)\n",
    "            preds = rbf_svm_clf.predict(X_validate_transformed)\n",
    "            cf = confusion_matrix(y_validate, preds)\n",
    "            if cf[1][1] > 100:\n",
    "                if cf[1][1] / (cf[1][1] + cf[0][1]) > best_precision:\n",
    "                    best_cf = cf\n",
    "                    best_precision = cf[1][1] / (cf[1][1] + cf[0][1])\n",
    "                    best_c = c\n",
    "                    best_g = g\n",
    "            print(f'done with c={c}, g={g}')\n",
    "    print(best_cf)\n",
    "    print(best_precision)\n",
    "    print(best_c)\n",
    "    print(best_g)\n",
    "rbf_svc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I tried using the sigmoid kernel:\n",
    "$$\\phi_{r, \\gamma} (\\textbf{x}, \\ell) = \\tanh (\\gamma \\cdot \\textbf{x}^T \\ell + r)$$\n",
    "which has a similar motivation to the sigmoid function from logistic regression.\n",
    "\n",
    "The best result here was: a probability of 41.5% with c=10 and gamma=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with c=10, g=0.1\n",
      "done with c=10, g=1\n",
      "done with c=10, g=10\n",
      "done with c=10, g=100\n",
      "done with c=10, g=1000\n",
      "done with c=10, g=10000\n",
      "done with c=10, g=100000\n",
      "done with c=100, g=0.1\n",
      "done with c=100, g=1\n",
      "done with c=100, g=10\n",
      "done with c=100, g=100\n",
      "done with c=100, g=1000\n",
      "done with c=100, g=10000\n",
      "done with c=100, g=100000\n",
      "done with c=1000, g=0.1\n",
      "done with c=1000, g=1\n",
      "done with c=1000, g=10\n",
      "done with c=1000, g=100\n",
      "done with c=1000, g=1000\n",
      "done with c=1000, g=10000\n",
      "done with c=1000, g=100000\n",
      "done with c=10000, g=0.1\n",
      "done with c=10000, g=1\n",
      "done with c=10000, g=10\n",
      "done with c=10000, g=100\n",
      "done with c=10000, g=1000\n",
      "done with c=10000, g=10000\n",
      "done with c=10000, g=100000\n",
      "done with c=100000, g=0.1\n",
      "done with c=100000, g=1\n",
      "done with c=100000, g=10\n",
      "done with c=100000, g=100\n",
      "done with c=100000, g=1000\n",
      "done with c=100000, g=10000\n",
      "done with c=100000, g=100000\n",
      "[[411 189]\n",
      " [202 134]]\n",
      "0.4148606811145511\n",
      "10\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "best_precision = 0\n",
    "for c in [10**i for i in range(1, 6)]:\n",
    "    for g in [10**i for i in range(-1, 6)]:\n",
    "        rbf_kernel_svm_clf = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('svm_clf', SVC(kernel='sigmoid', gamma=g, C=c, random_state=101))\n",
    "        ])\n",
    "        rbf_kernel_svm_clf.fit(X_train, y_train)\n",
    "        preds = rbf_kernel_svm_clf.predict(X_validate)\n",
    "        cf = confusion_matrix(y_validate, preds)\n",
    "        if cf[1][1] > 100:\n",
    "            if cf[1][1] / (cf[1][1] + cf[0][1]) > best_precision:\n",
    "                best_cf = cf\n",
    "                best_precision = cf[1][1] / (cf[1][1] + cf[0][1])\n",
    "                best_c = c\n",
    "                best_g = g\n",
    "        print(f'done with c={c}, g={g}')\n",
    "print(best_cf)\n",
    "print(best_precision)\n",
    "print(best_c)\n",
    "print(best_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the best performing model here was with the sigmoid kernel with polynomial features, with parameters c=10 and gamma=1. Let's assess it's test accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test precision: 0.3333333333333333\n",
      "[[420 212]\n",
      " [198 106]]\n"
     ]
    }
   ],
   "source": [
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def test_svc():\n",
    "    sigmoid_svm_clf = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('svm_clf', SVC(kernel='sigmoid', gamma=1, C=10, random_state=101))\n",
    "    ])\n",
    "    sigmoid_svm_clf.fit(X_train.append(X_validate), y_train.append(y_validate))\n",
    "    preds =sigmoid_svm_clf.predict(X_test)\n",
    "    cf = confusion_matrix(y_test, preds)\n",
    "    print(f'test precision: {cf[1][1] / (cf[1][1] + cf[0][1])}')\n",
    "    print(cf)\n",
    "test_svc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, a dissapointing result. Only 33.3% accuracy on the test data. 30% of games are worth watching so this is not performing any better than random guessing. Support vector machine classifiers have not performed well on the dataset. This is a classic case of the model overfitting the training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
