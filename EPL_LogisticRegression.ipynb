{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data and Discarding What is Not Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./clean_data.csv',sep='\\t',encoding='utf-8',index_col=0)\n",
    "data.drop(labels=['Date','HomeTeam','AwayTeam',\n",
    "                  'FTHG','FTAG','HS','AS','HST',\n",
    "                  'AST','HTHG','HTAG','HF','AF',\n",
    "                  'FTR','HTR','HY','AY','HR',\n",
    "                  'AR','Season','WHH','WHD','WHA'],\n",
    "         inplace=True,\n",
    "         axis=1)\n",
    "# Redorder the columns so we have home statistics, then away statistics\n",
    "# then betting odds and finally the classifier.\n",
    "data = data[['HGS','HGA','HYC','HRC','HWW','AGS','AGA','AYC','ARC','AWW','watch']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We insert a column of ones at the beginning of the DataFrame to allow the regression to fit with an constant term, also know as the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['intercept'] = np.ones((data.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6246, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Variables and the Problem at Hand\n",
    "\n",
    "There are going to be a lot of variables to keep track of so let's carefully define them and explain what they are in the context of the data above.\n",
    "\n",
    "Let the matrix $X$, represent the data consisting of $m$ observations of $d$ continuous predictors. In our case, $X$ is the dataframe above **without** the 'watch' column. For us, $X$ will be a $6246 \\times 11$ matrix, remembering we have added a column of ones to allow an intercept to be fitted.\n",
    "\n",
    "Let $y$ represent the column vector of labels with $m$ rows. In our case, this is the 'watch' column of the dataframe with $6246$ entries.\n",
    "\n",
    "Let $\\theta$ also be a column vector, with $d$  entries. In our case $d = 11$.\n",
    "\n",
    "A hyperplane is then defined by $\\theta^Tx = 0$, where $x^T = (1, x_1, x_2, ... , x_d)$ is a general point in $d$-space, which splits the space into two parts. We can then classify points based on what side of the plane they are on. The 2d example of this is $\\theta^T = (a, b, c)$ and $x^T = (1, x, y)$ where $\\theta^Tx = a + bx + by = 0$ defines a line splitting the plane into 2 parts.\n",
    "\n",
    "The problem at hand is to find values for the $\\theta$ vector which split the space into two parts to match our binary classifier.\n",
    "\n",
    "This is called a linear discriminant because we split the space using a plane with no curvature. This is the reason that the regression isn't going to perform particularly well. There is no reason to assume that the football data can be split by a non-curved boundary hyperplane.\n",
    "\n",
    "An optimal solution for $\\theta$ always exists to maximize correct prediction but it is computationally expensive to compute. Rather than finding the best values for $\\theta$ we incrementally improve an initial guess moving closer to the optimal solution using a technqiue called gradient descent.\n",
    "\n",
    "Imagine an egg cup where the optimal solution is the lowest point in the bowl. We start by randomly choosing a point on the bowl. By finding the gradient at that point we know the direction needed to travel towards the bottom. We move our position by some small amount (referred to as ALPHA in this notebook) towards the bottom of the egg cup. Then we recalculate the gradient and repeat. Does the name gradent descent make sense now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Logistic Regression\n",
    "\n",
    "A [sigmoid function](https://www.wikiwand.com/en/Sigmoid_function) is needed to ensure values between 0 and 1 are generated to represent probability. We use the logistic sigmoid function below.\n",
    "\n",
    "$$S(Y) = \\frac{1}{1 + e^{-Y}}$$\n",
    "\n",
    "We define it now to be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies the sigmoid function elementwise\n",
    "def sigmoid(Y):\n",
    "    return 1.0 / (1.0 + np.exp(-Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A logisitic regression is fit by *maximum likelihood* which means maximizing the probability that the correct labels are found given the training data and regression coefficients. This means we are maximizing $P(y|X,\\theta)$.\n",
    "\n",
    "For the regression to be fit, we need to introduce a penalty for misclassification that can then be minimized. This is called a cost function which we refer to as $C$. Here is the cost for a certain vector $\\theta$ missclassifying:\n",
    "\n",
    "$C(\\theta) = -\\log(S(X\\theta))$ if $y = 1$\n",
    "\n",
    "$C(\\theta) = -\\log(1 - S(X\\theta))$ if $y = 0$\n",
    "\n",
    "This cost functions punishes predictions close to 0 for rows with a prediction of 1 and vice versa. For example, if $S(X\\theta)$ is close to zero meaning the logistic regression predicts a classification of $0$ but actualy $y=1$ then $-\\log(S(X\\theta))$ is going to be a large value which tends to infinity as the predcition gets closer to zero (ie worse and worse).\n",
    "\n",
    "By assuming that all the joint observations of predictor and labels are independent and identically distributed (i.i.d), this definition leads to the log-likelihood equation becoming\n",
    "\n",
    "$\n",
    "\\begin{eqnarray}\n",
    " P(y|X,\\theta) & = & P(y_0, y_1, \\cdots , y_{m-1}|x_0, x_1, \\cdots , x_{m-1}, \\theta) \\\\\n",
    " & = & P(y_0|x_0,\\theta) P(y_1|x_1,\\theta) \\cdots P(y_{m-1}|x_{m-1},\\theta) \\\\\n",
    " & = & \\prod_{i=0}^{m-1}P(y_i|x_i,\\theta) \\\\\n",
    "\\end{eqnarray}\n",
    "$\n",
    "\n",
    "and as we use the *log*-likelihood, this results in\n",
    "\n",
    "$\n",
    "\\begin{eqnarray}\n",
    " L(\\theta ; y, X) & = & \\log \\left\\{ \\prod_{i=0}^{m-1}P(y_i|x_i,\\theta) \\right\\} \\\\\n",
    " & = & \\sum_{i=0}^{m-1} \\log (P(y_i|x_i,\\theta))\n",
    "\\end{eqnarray}\n",
    "$\n",
    "\n",
    "Under the assumption that $S$ is a good function to model probability and the i.i.d assumption above, the log-likelihood equation can be equivalently written in vector notation to avoid the use of a summation as\n",
    "\n",
    "$$L(\\theta) = y^TX\\theta + u^T\\ln S(-X\\theta)$$\n",
    "\n",
    "where $u$ is a row vector of $m$ ones.\n",
    "\n",
    "This function is defined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies log-likelihood function to theta, given \n",
    "# y (classes) and X (input data)\n",
    "def log_likelihood(theta, y, X):\n",
    "    u = np.ones((len(X), 1))\n",
    "    z = X @ theta\n",
    "    return y.T @ z + u.T @ (np.log(sigmoid(-z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the log likelihood equation is needed to perform gradient descent. Due to the fact that the cost function is convex, gradient descent is guaranteed to find the global minimum eventually. But it may take far too long in practice so we iterate for as long as we can in the situation. Here is the gradient:\n",
    "\n",
    "$$\\nabla_\\theta L(\\theta) = X^T [ y - G(X\\theta) ]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_log_likelihood(theta, y, X):\n",
    "    error = y - sigmoid(X @ theta)\n",
    "    return X.T.dot(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are reading to perform gradient descent. We allow the setting of parameters for ALPHA the amount we move in each iteration, often called the learning rate and MAX_STEPS the number of times we run the iteration. This function outputs an approximation of theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, y, ALPHA=0.1, MAX_STEPS=250):\n",
    "    y = y.astype(dtype=float)\n",
    "    \n",
    "    # Initialize theta as zeros\n",
    "    theta = np.zeros(X.shape[1])\n",
    "\n",
    "    # Using gradient descent incrementally improve\n",
    "    # the theta values towards the optimum solution\n",
    "    for t in range(MAX_STEPS):\n",
    "        delta_t = grad_log_likelihood(theta, y, X)\n",
    "        delta_t = delta_t / np.linalg.norm(delta_t, ord=2)\n",
    "        theta = theta + ALPHA * delta_t        \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the Regression\n",
    "\n",
    "First we split the data into training and testing. We will fit to the training data and then evaluate the accuracy by predicting the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4996, 12)\n",
      "(1250, 12)\n"
     ]
    }
   ],
   "source": [
    "random.seed(314) # Pi\n",
    "train_indices = random.sample(range(len(data)), int(0.8 * len(data)))\n",
    "test_indices = [i for i in range(len(data)) if i not in train_indices]\n",
    "train = data.iloc[train_indices]\n",
    "test = data.iloc[test_indices]\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the logistic regression function on the predictor columns and the result column. I have set MAX_STEPS as high as I am willing to wait for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = logistic_regression(train.drop('watch', axis=1),\n",
    "                            train['watch'],\n",
    "                            ALPHA=0.01,\n",
    "                            MAX_STEPS=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are coefficients of the regression equation. Note that the variables which seem to have the greatest influence on the prediciton are HWW and AWW. This makes sense as these are the proportion of past games in the season that have been worth watching!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HGS          0.050578\n",
      "HGA         -0.061416\n",
      "HYC          0.044441\n",
      "HRC          0.146679\n",
      "HWW          0.506910\n",
      "AGS          0.014740\n",
      "AGA         -0.067408\n",
      "AYC         -0.037042\n",
      "ARC          0.097068\n",
      "AWW          0.443800\n",
      "intercept   -0.933445\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the probability of each game being worth watching by multiplying the test data by the regression coefficients and applying the sigmoid function to restrict the values between 0 and 1. Unfortunately, the probabilities seems to be almost all between 0.2 and 0.4. It doesn't look like there are many games at all that the logistic regression was confident of being exciting. In fact it is normal to classify rows with a probability over 0.5 as being 1 and 0 otherwise. Well for this data, we would only predict one game as being probably worth watching!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4599    0.323108\n",
      "5792    0.331601\n",
      "7       0.441372\n",
      "5701    0.344439\n",
      "145     0.343601\n",
      "2361    0.332289\n",
      "4096    0.430158\n",
      "3289    0.332799\n",
      "639     0.324616\n",
      "6115    0.371568\n",
      "4612    0.313670\n",
      "1263    0.346605\n",
      "4044    0.376662\n",
      "518     0.340668\n",
      "3647    0.331257\n",
      "5116    0.334173\n",
      "573     0.338910\n",
      "3546    0.356114\n",
      "1877    0.325884\n",
      "734     0.357390\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "scores = test.drop('watch', axis=1) @ theta\n",
    "predictions = sigmoid(scores)\n",
    "\n",
    "print(predictions.sample(20, random_state=101))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the correctness of my implementation I will use the LogisticRegression function from Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=False,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=10000000,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=101, solver='saga', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use the 'saga' solver which the documentation recommends for large\n",
    "# datasets and we set the max iterations to be the same as above.\n",
    "clf = LogisticRegression(solver='saga',max_iter=10000000, fit_intercept=False, random_state=101)\n",
    "clf.fit(train.drop('watch', axis=1), train['watch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very close (though not equal) regression coefficients are found which shows that my implementation is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SK: 0.04719766, Me: 0.05057817\n",
      "SK: -0.06386703, Me: -0.06141576\n",
      "SK: 0.03659465, Me: 0.04444093\n",
      "SK: 0.133882, Me: 0.14667883\n",
      "SK: 0.48749553, Me: 0.50691018\n",
      "SK: 0.01030128, Me: 0.01473978\n",
      "SK: -0.07179634, Me: -0.06740772\n",
      "SK: -0.04433594, Me: -0.03704187\n",
      "SK: 0.08878757, Me: 0.09706804\n",
      "SK: 0.43073881, Me: 0.4438\n",
      "SK: -0.89385275, Me: -0.93344532\n"
     ]
    }
   ],
   "source": [
    "for z in zip(np.round((clf.coef_[0] + clf.intercept_), decimals=8), \n",
    "             list(np.round(theta, decimals=8))):\n",
    "    print(f'SK: {z[0]}, Me: {z[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = clf.predict_proba(test.drop('watch', axis=1))\n",
    "probs = [prob[1] for prob in probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[825, 423],\n",
       "       [  1,   1]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = clf.predict(test.drop('watch', axis=1))\n",
    "results = pd.DataFrame(data={'probs': probs, 'original': test['watch']})\n",
    "results['prediction'] = results['probs'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "print((results['original'] == results['prediction']).sum())\n",
    "confusion_matrix(results['prediction'], results['original'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, unfortunately only 1 game is predicted to be worth watching! One way around this is to perhaps just watch games that the logistic regression predicts to be exciting with probability above a certain threshold. Or look at the probabilities of all the games being exciting to watch each weekend and watch the most likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[615 285]\n",
      " [211 139]]\n",
      "754\n"
     ]
    }
   ],
   "source": [
    "alt_results = pd.DataFrame(data={'probs': probs, 'original': test['watch']})\n",
    "alt_results['preds'] = alt_results['probs'].apply(lambda x: 1 if x > 0.35 else 0)\n",
    "print(confusion_matrix(alt_results['preds'], alt_results['original']))\n",
    "print((alt_results['original'] == alt_results['preds']).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, more games are predicted to be worth watching if we lower thethreshold probability but we have introduced many false positives into the predictions.\n",
    "\n",
    "As outlined in my blogpost, it seems that these predictors are exhibited evidence of collinearity. Therefore, it may be beneficial to drop some of the attributes and train again. I kept just HWW and AWW and retrained below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=10000000,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=101, solver='saga', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ww = train[['HWW', 'AWW', 'watch']]\n",
    "test_ww = test[['HWW', 'AWW', 'watch']]\n",
    "clf_ww = LogisticRegression(solver='saga',max_iter=10000000, \n",
    "                            fit_intercept=True,\n",
    "                           random_state=101)\n",
    "clf_ww.fit(train_ww.drop('watch', axis=1), train_ww['watch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_ww = clf_ww.predict_proba(test_ww.drop('watch', axis=1))\n",
    "probs_ww = [prob[1] for prob in probs_ww]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4730943548442019\n",
      "0.2745230581583436\n"
     ]
    }
   ],
   "source": [
    "print(max(probs_ww))\n",
    "print(min(probs_ww))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[615 285]\n",
      " [211 139]]\n"
     ]
    }
   ],
   "source": [
    "alt_results_ww = pd.DataFrame(data={'probs': probs_ww, 'original': test_ww['watch']})\n",
    "alt_results_ww['preds'] = alt_results['probs'].apply(lambda x: 1 if x > 0.35 else 0)\n",
    "print(confusion_matrix(alt_results_ww['preds'],alt_results_ww['original']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that still we are not getting very good results. We are predicting with about 60.5% accuracy and when we predict a game will be worth watching there is a 32.8% chance that it will actually be. Again this is only slightly better than random guessing as about 30% of games are worth watching. So a different approach is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
